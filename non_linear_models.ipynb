{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doğrusal Olmayan Regresyon Modelleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Hitters.csv\")\n",
    "df = df.dropna()\n",
    "df = pd.get_dummies(df, columns = ['League', 'Division', 'NewLeague'], drop_first = True)\n",
    "y = df[\"Salary\"]\n",
    "X = df.drop('Salary', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>League_N</th>\n",
       "      <th>Division_W</th>\n",
       "      <th>NewLeague_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "5    594   169      4    74   51     35     11    4408   1133      19    501   \n",
       "\n",
       "   CRBI  CWalks  PutOuts  Assists  Errors  League_N  Division_W  NewLeague_N  \n",
       "1   414     375      632       43      10         1           1            1  \n",
       "2   266     263      880       82      14         0           1            0  \n",
       "3   838     354      200       11       3         1           0            1  \n",
       "4    46      33      805       40       4         1           0            1  \n",
       "5   336     194      282      421      25         0           1            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442.7695798487812"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = knn_model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hatırlatma\n",
    "\n",
    "- train-test\n",
    "\n",
    "- tum veri ile cv (gormediği verideki performans?)\n",
    "\n",
    "- train-test (train uzerine cv, test ile test edilir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2 için RMSE değeri: 402.3490108489193\n",
      "k = 3 için RMSE değeri: 420.8851794272263\n",
      "k = 4 için RMSE değeri: 433.042118061226\n",
      "k = 5 için RMSE değeri: 438.75715389787524\n",
      "k = 6 için RMSE değeri: 447.4667559140737\n",
      "k = 7 için RMSE değeri: 453.52179196594597\n",
      "k = 8 için RMSE değeri: 454.3240838534937\n",
      "k = 9 için RMSE değeri: 459.34454670250665\n",
      "k = 10 için RMSE değeri: 453.9051617388242\n",
      "k = 11 için RMSE değeri: 446.22600056274575\n",
      "k = 12 için RMSE değeri: 450.1945294370967\n",
      "k = 13 için RMSE değeri: 451.09507598791004\n",
      "k = 14 için RMSE değeri: 451.4981224280202\n",
      "k = 15 için RMSE değeri: 453.8713948576225\n",
      "k = 16 için RMSE değeri: 451.7888216207132\n",
      "k = 17 için RMSE değeri: 452.35446754486134\n",
      "k = 18 için RMSE değeri: 442.2032272885196\n",
      "k = 19 için RMSE değeri: 442.68511238980847\n",
      "k = 20 için RMSE değeri: 442.7695798487812\n",
      "k = 21 için RMSE değeri: 440.6012781451951\n"
     ]
    }
   ],
   "source": [
    "RMSE = []\n",
    "\n",
    "for k in range(20):\n",
    "    k = k + 2\n",
    "    knn_model = KNeighborsRegressor(n_neighbors = k).fit(X_train, y_train)\n",
    "    y_pred = knn_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    RMSE.append(rmse)\n",
    "    print(\"k =\", k, \"için RMSE değeri:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\"n_neighbors\": np.arange(2,30,1)}\n",
    "\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "knn_cv_model = GridSearchCV(knn_model, knn_params, cv = 10).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 4}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tuned = KNeighborsRegressor(**knn_cv_model.best_params_).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433.042118061226"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Hitters.csv\")\n",
    "df = df.dropna()\n",
    "df = pd.get_dummies(df, columns = ['League', 'Division', 'NewLeague'], drop_first = True)\n",
    "y = df[\"Salary\"]\n",
    "X = df.drop('Salary', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model = SVR(\"linear\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "    kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488.55774927996595"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svr_model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  40 | elapsed:   31.9s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   33.9s finished\n"
     ]
    }
   ],
   "source": [
    "svr_model = SVR(\"linear\") \n",
    "\n",
    "svr_params = {\"C\": [0.01,0.001, 0.2, 0.1,0.5,0.8,0.9,1]}\n",
    "\n",
    "svr_cv_model = GridSearchCV(svr_model, svr_params, cv = 5, n_jobs = -1, verbose =  2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_tuned = SVR(\"linear\", C = 0.001).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488.7612678891111"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svr_tuned.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshrinking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Epsilon-Support Vector Regression.\n",
       "\n",
       "The free parameters in the model are C and epsilon.\n",
       "\n",
       "The implementation is based on libsvm. The fit time complexity\n",
       "is more than quadratic with the number of samples which makes it hard\n",
       "to scale to datasets with more than a couple of 10000 samples. For large\n",
       "datasets consider using :class:`sklearn.svm.LinearSVR` or\n",
       ":class:`sklearn.linear_model.SGDRegressor` instead, possibly after a\n",
       ":class:`sklearn.kernel_approximation.Nystroem` transformer.\n",
       "\n",
       "Read more in the :ref:`User Guide <svm_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "kernel : string, optional (default='rbf')\n",
       "     Specifies the kernel type to be used in the algorithm.\n",
       "     It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
       "     a callable.\n",
       "     If none is given, 'rbf' will be used. If a callable is given it is\n",
       "     used to precompute the kernel matrix.\n",
       "\n",
       "degree : int, optional (default=3)\n",
       "    Degree of the polynomial kernel function ('poly').\n",
       "    Ignored by all other kernels.\n",
       "\n",
       "gamma : {'scale', 'auto'} or float, optional (default='scale')\n",
       "    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
       "\n",
       "    - if ``gamma='scale'`` (default) is passed then it uses\n",
       "      1 / (n_features * X.var()) as value of gamma,\n",
       "    - if 'auto', uses 1 / n_features.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
       "\n",
       "coef0 : float, optional (default=0.0)\n",
       "    Independent term in kernel function.\n",
       "    It is only significant in 'poly' and 'sigmoid'.\n",
       "\n",
       "tol : float, optional (default=1e-3)\n",
       "    Tolerance for stopping criterion.\n",
       "\n",
       "C : float, optional (default=1.0)\n",
       "    Regularization parameter. The strength of the regularization is\n",
       "    inversely proportional to C. Must be strictly positive.\n",
       "    The penalty is a squared l2 penalty.\n",
       "\n",
       "epsilon : float, optional (default=0.1)\n",
       "     Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n",
       "     within which no penalty is associated in the training loss function\n",
       "     with points predicted within a distance epsilon from the actual\n",
       "     value.\n",
       "\n",
       "shrinking : boolean, optional (default=True)\n",
       "    Whether to use the shrinking heuristic.\n",
       "\n",
       "cache_size : float, optional\n",
       "    Specify the size of the kernel cache (in MB).\n",
       "\n",
       "verbose : bool, default: False\n",
       "    Enable verbose output. Note that this setting takes advantage of a\n",
       "    per-process runtime setting in libsvm that, if enabled, may not work\n",
       "    properly in a multithreaded context.\n",
       "\n",
       "max_iter : int, optional (default=-1)\n",
       "    Hard limit on iterations within solver, or -1 for no limit.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "support_ : array-like of shape (n_SV)\n",
       "    Indices of support vectors.\n",
       "\n",
       "support_vectors_ : array-like of shape (n_SV, n_features)\n",
       "    Support vectors.\n",
       "\n",
       "dual_coef_ : array, shape = [1, n_SV]\n",
       "    Coefficients of the support vector in the decision function.\n",
       "\n",
       "coef_ : array, shape = [1, n_features]\n",
       "    Weights assigned to the features (coefficients in the primal\n",
       "    problem). This is only available in the case of a linear kernel.\n",
       "\n",
       "    `coef_` is readonly property derived from `dual_coef_` and\n",
       "    `support_vectors_`.\n",
       "\n",
       "fit_status_ : int\n",
       "    0 if correctly fitted, 1 otherwise (will raise warning)\n",
       "\n",
       "intercept_ : array, shape = [1]\n",
       "    Constants in decision function.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.svm import SVR\n",
       ">>> import numpy as np\n",
       ">>> n_samples, n_features = 10, 5\n",
       ">>> rng = np.random.RandomState(0)\n",
       ">>> y = rng.randn(n_samples)\n",
       ">>> X = rng.randn(n_samples, n_features)\n",
       ">>> clf = SVR(C=1.0, epsilon=0.2)\n",
       ">>> clf.fit(X, y)\n",
       "SVR(epsilon=0.2)\n",
       "\n",
       "See also\n",
       "--------\n",
       "NuSVR\n",
       "    Support Vector Machine for regression implemented using libsvm\n",
       "    using a parameter to control the number of support vectors.\n",
       "\n",
       "LinearSVR\n",
       "    Scalable Linear Support Vector Machine for regression\n",
       "    implemented using liblinear.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "**References:**\n",
       "`LIBSVM: A Library for Support Vector Machines\n",
       "<http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`__\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.7/site-packages/sklearn/svm/_classes.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    1.8s finished\n"
     ]
    }
   ],
   "source": [
    "#nonlinear\n",
    "svr_model = SVR() \n",
    "\n",
    "svr_params = {\"C\": [0.01,0.001, 0.2, 0.1,0.5,0.8,0.9,1, 10, 100, 500,1000]}\n",
    "\n",
    "svr_cv_model = GridSearchCV(svr_model, svr_params, cv = 5, n_jobs = -1, verbose =  2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_tuned = SVR(**svr_cv_model.best_params_).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434.90979834824327"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svr_tuned.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yapay Sinir Ağları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Hitters.csv\")\n",
    "df = df.dropna()\n",
    "df = pd.get_dummies(df, columns = ['League', 'Division', 'NewLeague'], drop_first = True)\n",
    "y = df[\"Salary\"]\n",
    "X = df.drop('Salary', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_test)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPRegressor().fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        MLPRegressor\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "           beta_2= <...> ,\n",
       "           tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "           warm_start=False)\n",
       "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Multi-layer Perceptron regressor.\n",
       "\n",
       "This model optimizes the squared-loss using LBFGS or stochastic gradient\n",
       "descent.\n",
       "\n",
       ".. versionadded:: 0.18\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)\n",
       "    The ith element represents the number of neurons in the ith\n",
       "    hidden layer.\n",
       "\n",
       "activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n",
       "    Activation function for the hidden layer.\n",
       "\n",
       "    - 'identity', no-op activation, useful to implement linear bottleneck,\n",
       "      returns f(x) = x\n",
       "\n",
       "    - 'logistic', the logistic sigmoid function,\n",
       "      returns f(x) = 1 / (1 + exp(-x)).\n",
       "\n",
       "    - 'tanh', the hyperbolic tan function,\n",
       "      returns f(x) = tanh(x).\n",
       "\n",
       "    - 'relu', the rectified linear unit function,\n",
       "      returns f(x) = max(0, x)\n",
       "\n",
       "solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n",
       "    The solver for weight optimization.\n",
       "\n",
       "    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
       "\n",
       "    - 'sgd' refers to stochastic gradient descent.\n",
       "\n",
       "    - 'adam' refers to a stochastic gradient-based optimizer proposed by\n",
       "      Kingma, Diederik, and Jimmy Ba\n",
       "\n",
       "    Note: The default solver 'adam' works pretty well on relatively\n",
       "    large datasets (with thousands of training samples or more) in terms of\n",
       "    both training time and validation score.\n",
       "    For small datasets, however, 'lbfgs' can converge faster and perform\n",
       "    better.\n",
       "\n",
       "alpha : float, default=0.0001\n",
       "    L2 penalty (regularization term) parameter.\n",
       "\n",
       "batch_size : int, default='auto'\n",
       "    Size of minibatches for stochastic optimizers.\n",
       "    If the solver is 'lbfgs', the classifier will not use minibatch.\n",
       "    When set to \"auto\", `batch_size=min(200, n_samples)`\n",
       "\n",
       "learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n",
       "    Learning rate schedule for weight updates.\n",
       "\n",
       "    - 'constant' is a constant learning rate given by\n",
       "      'learning_rate_init'.\n",
       "\n",
       "    - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
       "      at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
       "      effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
       "\n",
       "    - 'adaptive' keeps the learning rate constant to\n",
       "      'learning_rate_init' as long as training loss keeps decreasing.\n",
       "      Each time two consecutive epochs fail to decrease training loss by at\n",
       "      least tol, or fail to increase validation score by at least tol if\n",
       "      'early_stopping' is on, the current learning rate is divided by 5.\n",
       "\n",
       "    Only used when solver='sgd'.\n",
       "\n",
       "learning_rate_init : double, default=0.001\n",
       "    The initial learning rate used. It controls the step-size\n",
       "    in updating the weights. Only used when solver='sgd' or 'adam'.\n",
       "\n",
       "power_t : double, default=0.5\n",
       "    The exponent for inverse scaling learning rate.\n",
       "    It is used in updating effective learning rate when the learning_rate\n",
       "    is set to 'invscaling'. Only used when solver='sgd'.\n",
       "\n",
       "max_iter : int, default=200\n",
       "    Maximum number of iterations. The solver iterates until convergence\n",
       "    (determined by 'tol') or this number of iterations. For stochastic\n",
       "    solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
       "    (how many times each data point will be used), not the number of\n",
       "    gradient steps.\n",
       "\n",
       "shuffle : bool, default=True\n",
       "    Whether to shuffle samples in each iteration. Only used when\n",
       "    solver='sgd' or 'adam'.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    If int, random_state is the seed used by the random number generator;\n",
       "    If RandomState instance, random_state is the random number generator;\n",
       "    If None, the random number generator is the RandomState instance used\n",
       "    by `np.random`.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for the optimization. When the loss or score is not improving\n",
       "    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
       "    unless ``learning_rate`` is set to 'adaptive', convergence is\n",
       "    considered to be reached and training stops.\n",
       "\n",
       "verbose : bool, default=False\n",
       "    Whether to print progress messages to stdout.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous\n",
       "    call to fit as initialization, otherwise, just erase the\n",
       "    previous solution. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "momentum : float, default=0.9\n",
       "    Momentum for gradient descent update.  Should be between 0 and 1. Only\n",
       "    used when solver='sgd'.\n",
       "\n",
       "nesterovs_momentum : boolean, default=True\n",
       "    Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
       "    momentum > 0.\n",
       "\n",
       "early_stopping : bool, default=False\n",
       "    Whether to use early stopping to terminate training when validation\n",
       "    score is not improving. If set to true, it will automatically set\n",
       "    aside 10% of training data as validation and terminate training when\n",
       "    validation score is not improving by at least ``tol`` for\n",
       "    ``n_iter_no_change`` consecutive epochs.\n",
       "    Only effective when solver='sgd' or 'adam'\n",
       "\n",
       "validation_fraction : float, default=0.1\n",
       "    The proportion of training data to set aside as validation set for\n",
       "    early stopping. Must be between 0 and 1.\n",
       "    Only used if early_stopping is True\n",
       "\n",
       "beta_1 : float, default=0.9\n",
       "    Exponential decay rate for estimates of first moment vector in adam,\n",
       "    should be in [0, 1). Only used when solver='adam'\n",
       "\n",
       "beta_2 : float, default=0.999\n",
       "    Exponential decay rate for estimates of second moment vector in adam,\n",
       "    should be in [0, 1). Only used when solver='adam'\n",
       "\n",
       "epsilon : float, default=1e-8\n",
       "    Value for numerical stability in adam. Only used when solver='adam'\n",
       "\n",
       "n_iter_no_change : int, default=10\n",
       "    Maximum number of epochs to not meet ``tol`` improvement.\n",
       "    Only effective when solver='sgd' or 'adam'\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "max_fun : int, default=15000\n",
       "    Only used when solver='lbfgs'. Maximum number of function calls.\n",
       "    The solver iterates until convergence (determined by 'tol'), number\n",
       "    of iterations reaches max_iter, or this number of function calls.\n",
       "    Note that number of function calls will be greater than or equal to\n",
       "    the number of iterations for the MLPRegressor.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "loss_ : float\n",
       "    The current loss computed with the loss function.\n",
       "\n",
       "coefs_ : list, length n_layers - 1\n",
       "    The ith element in the list represents the weight matrix corresponding\n",
       "    to layer i.\n",
       "\n",
       "intercepts_ : list, length n_layers - 1\n",
       "    The ith element in the list represents the bias vector corresponding to\n",
       "    layer i + 1.\n",
       "\n",
       "n_iter_ : int,\n",
       "    The number of iterations the solver has ran.\n",
       "\n",
       "n_layers_ : int\n",
       "    Number of layers.\n",
       "\n",
       "n_outputs_ : int\n",
       "    Number of outputs.\n",
       "\n",
       "out_activation_ : string\n",
       "    Name of the output activation function.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "MLPRegressor trains iteratively since at each time step\n",
       "the partial derivatives of the loss function with respect to the model\n",
       "parameters are computed to update the parameters.\n",
       "\n",
       "It can also have a regularization term added to the loss function\n",
       "that shrinks model parameters to prevent overfitting.\n",
       "\n",
       "This implementation works with data represented as dense and sparse numpy\n",
       "arrays of floating point values.\n",
       "\n",
       "References\n",
       "----------\n",
       "Hinton, Geoffrey E.\n",
       "    \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
       "    (1989): 185-234.\n",
       "\n",
       "Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
       "    training deep feedforward neural networks.\" International Conference\n",
       "    on Artificial Intelligence and Statistics. 2010.\n",
       "\n",
       "He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
       "    performance on imagenet classification.\" arXiv preprint\n",
       "    arXiv:1502.01852 (2015).\n",
       "\n",
       "Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
       "    optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637.4452212655443"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mlp_model.predict(X_test_scaled)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_params = {\"alpha\": [0.1, 0.01, 0.02, 0.001, 0.0001], \n",
    "             \"hidden_layer_sizes\": [(10,20), (5,5), (100,100), (1000,100,10)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   41.0s finished\n"
     ]
    }
   ],
   "source": [
    "mlp_cv_model = GridSearchCV(mlp_model, mlp_params, cv = 10, verbose = 2, n_jobs = -1).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.01, 'hidden_layer_sizes': (1000, 100, 10)}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_tuned = MLPRegressor(**mlp_cv_model.best_params_).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453.50312597546514"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mlp_tuned.predict(X_test_scaled)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Hitters.csv\")\n",
    "df = df.dropna()\n",
    "df = pd.get_dummies(df, columns = ['League', 'Division', 'NewLeague'], drop_first = True)\n",
    "y = df[\"Salary\"]\n",
    "X = df.drop('Salary', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=52, splitter='best')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_model = DecisionTreeRegressor(random_state = 52)\n",
    "cart_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bir fonksiyonun türevinin sıfır olduğu nokta eğitimidir, gradyanıdır, bu noktanın negatif yonu ilgili fonksiyonu azaltır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407.7930907067807"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cart_model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agac gelistirirken verinin 3'te 2'si train için 3'te 1'i başarı değerlendirme için kullanılır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        DecisionTreeRegressor\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
       "           max_f <...> raction_leaf=0.0, presort='deprecated',\n",
       "           random_state=None, splitter='best')\n",
       "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "A decision tree regressor.\n",
       "\n",
       "Read more in the :ref:`User Guide <tree>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "criterion : {\"mse\", \"friedman_mse\", \"mae\"}, default=\"mse\"\n",
       "    The function to measure the quality of a split. Supported criteria\n",
       "    are \"mse\" for the mean squared error, which is equal to variance\n",
       "    reduction as feature selection criterion and minimizes the L2 loss\n",
       "    using the mean of each terminal node, \"friedman_mse\", which uses mean\n",
       "    squared error with Friedman's improvement score for potential splits,\n",
       "    and \"mae\" for the mean absolute error, which minimizes the L1 loss\n",
       "    using the median of each terminal node.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Mean Absolute Error (MAE) criterion.\n",
       "\n",
       "splitter : {\"best\", \"random\"}, default=\"best\"\n",
       "    The strategy used to choose the split at each node. Supported\n",
       "    strategies are \"best\" to choose the best split and \"random\" to choose\n",
       "    the best random split.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `int(max_features * n_features)` features are considered at each\n",
       "      split.\n",
       "    - If \"auto\", then `max_features=n_features`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "random_state : int or RandomState, default=None\n",
       "    If int, random_state is the seed used by the random number generator;\n",
       "    If RandomState instance, random_state is the random number generator;\n",
       "    If None, the random number generator is the RandomState instance used\n",
       "    by `np.random`.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "min_impurity_split : float, (default=1e-7)\n",
       "    Threshold for early stopping in tree growth. A node will split\n",
       "    if its impurity is above the threshold, otherwise it is a leaf.\n",
       "\n",
       "    .. deprecated:: 0.19\n",
       "       ``min_impurity_split`` has been deprecated in favor of\n",
       "       ``min_impurity_decrease`` in 0.19. The default value of\n",
       "       ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
       "       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
       "\n",
       "presort : deprecated, default='deprecated'\n",
       "    This parameter is deprecated and will be removed in v0.24.\n",
       "\n",
       "    .. deprecated:: 0.22\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the\n",
       "    (normalized) total reduction of the criterion brought\n",
       "    by that feature. It is also known as the Gini importance [4]_.\n",
       "\n",
       "max_features_ : int\n",
       "    The inferred value of max_features.\n",
       "\n",
       "n_features_ : int\n",
       "    The number of features when ``fit`` is performed.\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "tree_ : Tree\n",
       "    The underlying Tree object. Please refer to\n",
       "    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
       "    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
       "    for basic usage of these attributes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DecisionTreeClassifier : A decision tree classifier.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data and\n",
       "``max_features=n_features``, if the improvement of the criterion is\n",
       "identical for several splits enumerated during the search of the best\n",
       "split. To obtain a deterministic behaviour during fitting,\n",
       "``random_state`` has to be fixed.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       ".. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
       "\n",
       ".. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
       "       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
       "\n",
       ".. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
       "       Learning\", Springer, 2009.\n",
       "\n",
       ".. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
       "       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_boston\n",
       ">>> from sklearn.model_selection import cross_val_score\n",
       ">>> from sklearn.tree import DecisionTreeRegressor\n",
       ">>> X, y = load_boston(return_X_y=True)\n",
       ">>> regressor = DecisionTreeRegressor(random_state=0)\n",
       ">>> cross_val_score(regressor, X, y, cv=10)\n",
       "...                    # doctest: +SKIP\n",
       "...\n",
       "array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,\n",
       "        0.07..., 0.29..., 0.33..., -1.42..., -1.77...])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?cart_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_params = {\"max_depth\": [2,3,4,5,10,20, 100, 1000],\n",
    "              \"min_samples_split\": [2,10,5,30,50,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_model = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_cv_model = GridSearchCV(cart_model, cart_params, cv = 10).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4, 'min_samples_split': 50}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_tuned = DecisionTreeRegressor(**cart_cv_model.best_params_).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.66820692864223"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cart_tuned.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
